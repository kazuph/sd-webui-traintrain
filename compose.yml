services:
  app:
    tty: true # インタラクティブな操作のために残す
    build:
      context: .
      dockerfile: "Dockerfile"
    # ポート公開は cli.py を直接実行する場合は通常不要
    # ports:
    #   - "8000:8000"
    environment:
      - NVIDIA_VISIBLE_DEVICES=0 # 使用するGPU ID (環境に合わせて変更)
      - HF_HOME=/app/cache # Hugging Face キャッシュディレクトリ
      - CC=/usr/bin/gcc # Cコンパイラのパスを実行時に設定
      - STABLE_DIFFUSION_MODEL_DIR=/models
      - STABLE_DIFFUSION_MODEL_FILENAME=${STABLE_DIFFUSION_MODEL_FILENAME}
    volumes:
      # モデルファイルをコンテナ内の /models にマウント (読み取り専用)
      - ${STABLE_DIFFUSION_MODEL_DIR}/${STABLE_DIFFUSION_MODEL_FILENAME}:/models/${STABLE_DIFFUSION_MODEL_FILENAME}:ro
      # inputs ディレクトリをコンテナ内の /app/inputs にマウント (読み取り専用)
      - ./inputs:/app/inputs:ro
      # outputs ディレクトリをコンテナ内の /outputs にマウント (読み書き可能)
      - ./outputs:/app/outputs:rw
      # 必要であればキャッシュもマウント (コメントアウト中)
      # - ./cache:/app/cache:rw # Hugging Face キャッシュ (ホストと共有する場合)
    deploy:
      resources:
        reservations:
          devices:
              - driver: nvidia
                device_ids: ['0'] # 使用するGPU ID (環境に合わせて変更)
                capabilities: [gpu]
